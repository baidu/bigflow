/***************************************************************************
 *
 * Copyright (c) 2016 Baidu, Inc. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 **************************************************************************/
// Author: Wang Cong <wangcong09@baidu.com>

#include "flume/runtime/spark/spark_job.h"

#include <cstdlib>
#include <iostream>  // NOLINT
#include <vector>

#include "thirdparty/boost/filesystem.hpp"
#include "thirdparty/boost/foreach.hpp"
#include "thirdparty/boost/lexical_cast.hpp"
#include "thirdparty/boost/foreach.hpp"
#include "thirdparty/gflags/gflags.h"
#include "thirdparty/re2/re2/re2.h"
#include "toft/base/array_size.h"
#include "toft/base/string/algorithm.h"
#include "toft/crypto/uuid/uuid.h"
#include "toft/storage/path/path.h"

#include "flume/proto/logical_plan.pb.h"
#include "flume/proto/physical_plan.pb.h"
#include "flume/core/entity.h"
#include "flume/core/sinker.h"
#include "flume/runtime/backend.h"
#include "flume/util/config_util.h"
#include "flume/util/jni_environment.h"

namespace baidu {
namespace flume {
namespace runtime {
namespace spark {

using core::Entity;
using core::Sinker;
using re2::RE2;

const std::string SparkJob::kSparkJarName = "spark_launcher.jar";

static const char* SPARK_JOB_CLASS = "com/baidu/flume/runtime/spark/SparkJob";
static const char* SPARK_JOB_CONSTRUCTOR_PARAM = "([BLjava/lang/String;)V";
static const char* SPARK_JOB_RUN_METHOD = "run";
static const char* SPARK_JOB_RUN_METHOD_PARAM = "()V";

using util::JniEnvironment;

class SparkJob::Impl {
public:
    Impl(const PbJob& job) : _job(job) {
        JniEnvironment jni_environment;
        _env = jni_environment.get_env();
        _j_spark_job_class = _env->FindClass(SPARK_JOB_CLASS);
        JniEnvironment::check_and_describe(_env, _j_spark_job_class);

        jmethodID j_spark_job_constructor = _env->GetMethodID(
                _j_spark_job_class,
                "<init>",
                SPARK_JOB_CONSTRUCTOR_PARAM);

        JniEnvironment::check_and_describe(_env, j_spark_job_constructor);

        LOG(INFO) << "Start make byte array";
        std::string job_string = _job.SerializeAsString();
        jsize len = job_string.length();
        jbyteArray j_job_byte_array = _env->NewByteArray(len);
        _env->SetByteArrayRegion(
                j_job_byte_array,
                0u,
                len,
                reinterpret_cast<jbyte*>(const_cast<char*>(job_string.data())));

        jstring j_archive_path = _env->NewStringUTF("");

        LOG(INFO) << "End making, construct";
        _j_spark_job = _env->NewObject(
                _j_spark_job_class,
                j_spark_job_constructor,
                j_job_byte_array,
                j_archive_path);
        JniEnvironment::check_and_describe(_env, _j_spark_job);
        LOG(INFO) << "Construct done";

        _env->DeleteLocalRef(j_job_byte_array);
        _env->DeleteLocalRef(j_archive_path);

        _j_spark_job_run_method = _env->GetMethodID(
                _j_spark_job_class,
                SPARK_JOB_RUN_METHOD,
                SPARK_JOB_RUN_METHOD_PARAM);
        JniEnvironment::check_and_describe(_env, _j_spark_job_run_method);
    }

    virtual ~Impl() {}

    bool run() {
        jboolean ret = _env->CallBooleanMethod(_j_spark_job, _j_spark_job_run_method);
        return ret;
    }

    bool kill() {}

    std::string job_id() { return ""; }

private:
    PbJob _job;

    JNIEnv* _env;

    jclass _j_spark_job_class;
    jobject _j_spark_job;
    jmethodID _j_spark_job_run_method;
};

SparkJob::SparkJob(const PbJob& job, const std::string& resource_path)
    : _impl(new SparkJob::Impl(job)) {
}

SparkJob::~SparkJob() {
}

bool SparkJob::run() {
    return _impl->run();
//    std::string spark_home = m_job.job_config().spark_home_path();
//    std::string spark_submit = spark_home + "/bin/spark_submit";
}

void SparkJob::kill() {
    _impl->kill();
//    while (!m_is_done && (m_job_id.empty() || m_job_tracker.empty())) {
//        sleep(1);
//    }
//
//    if (!m_is_done) {
//        m_client->Commit()
//            .WithArg("job")
//            .WithArg("-Dmapred.job.tracker=" + m_job_tracker)
//            .WithArg("-kill").WithArg(m_job_id);
//    }
}

// void SparkJob::ProcessClientOutput(const std::string& line) {
//     std::string job_id;
//     if (RE2::PartialMatch(line, *m_job_id_regex, &job_id)) {
//         LOG(INFO) << "Got hadoop job id: " << job_id;
//         m_job_id = job_id;
//     }
//
//     std::string tracking_url;
//     if (RE2::PartialMatch(line, *m_tracking_url_regex, &tracking_url)) {
//         LOG(INFO) << "Got tracking url: " << tracking_url;
//         m_tracking_url = tracking_url;
//     }
//
//     std::string job_tracker;
//     if (RE2::PartialMatch(line, *m_job_tracker_regex, &job_tracker)) {
//         LOG(INFO) << "Got job tracker: " << job_tracker;
//         m_job_tracker = job_tracker;
//     }
// }

std::string SparkJob::job_id() {
    return _impl->job_id();
}

}  // namespace spark
}  // namespace runtime
}  // namespace flume
}  // namespace baidu
